{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify input data files using Glob library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Exam1-fa19/datafiles-socrative/Class_09_12_2019__15_02_QZ_datascience20190912thu.xlsx',\n",
       " './Exam1-fa19/datafiles-socrative/Class_08_29_2019__14_52_QZ_datascience20190829thu.xlsx',\n",
       " './Exam1-fa19/datafiles-socrative/Class_09_17_2019__15_01_QZ_datascience20190917tuesday.xlsx',\n",
       " './Exam1-fa19/datafiles-socrative/Class_09_19_2019__15_01_QZ_datascience20190919thursday.xlsx',\n",
       " './Exam1-fa19/datafiles-socrative/Class_09_10_2019__15_00_QZ_datascience20190910.xlsx']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xl_files = glob('./Exam1-fa19/datafiles-socrative/*.xlsx', recursive=False)\n",
    "display(xl_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define necessary functions\n",
    "\n",
    "Since the code will run in a loop, much of the work will be pushed into functions which are called on each loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of default necessary columns \n",
    "necessary_cols = [\"Student Names\", \"Student ID\"]\n",
    "\n",
    "def identify_header_row(df, req_cols = necessary_cols):\n",
    "    \"\"\"\n",
    "    given a pandas dataframe, returns the row number which is presumably the header. \n",
    "    \n",
    "    req_cols: optional, list of strings used to identify the header row\n",
    "    \"\"\"\n",
    "    # make a condition which: \n",
    "    #     - selects all columns containing string\n",
    "    #     - stacks those columns into rows and checks any which are in req_cols\n",
    "    #     - returns the df to inital shape, as a boolean mask named condition\n",
    "    condition = df.select_dtypes(include=[object]).stack().isin(req_cols).unstack()\n",
    "    # fill any empty values in the conditional bool mask with \"False\"\n",
    "    condition = condition.fillna(False)\n",
    "    # select from df those data meeting boolean mask conditions\n",
    "    # and drop any rows which have ALL False, or NaN (row wise)\n",
    "    header_row = df[condition].dropna(how='all')\n",
    "    # Verify the quantity of rows is valid\n",
    "    header_row_qty = len(header_row)\n",
    "    if header_row_qty > 1:\n",
    "        raise Exception(f'Too many header rows!')\n",
    "    elif header_row_qty < 1:\n",
    "        raise Exception('No header rows found!')\n",
    "    # return the index position of the row as an int\n",
    "    return header_row.index.values[0] + 1\n",
    "\n",
    "def retrieve_longest_str(group):\n",
    "    \"\"\"\n",
    "    given a groupby series returns the longest string of the series\n",
    "    transformed to title case.\n",
    "    \"\"\"\n",
    "    # retrieve the largest result using the len function as the key to determine largest\n",
    "    try:\n",
    "        longest = max(group, key=len)\n",
    "        longest = longest.title()\n",
    "    except TypeError:\n",
    "        longest = group[0]\n",
    "    return longest\n",
    "\n",
    "def collapse_date_floats(group):\n",
    "    \"\"\"\n",
    "    given a groupby series of floats, returns 1 if any are 1 and\n",
    "    returns 0 if none are 1\n",
    "    \"\"\"\n",
    "    \n",
    "    if any(group) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def retrieve_common_name(group):\n",
    "    \"\"\"\n",
    "    give a groupby series of strings, returns the most frequent\n",
    "    \"\"\"\n",
    "    \n",
    "    return max(set(group), key=group.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load each file, run initial cleaning functions then concatenate them into a single (pre-cleaned) df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for filePath in xl_files:\n",
    "    path, fileName = os.path.split(filePath)\n",
    "       \n",
    "    # extract information based on expected file naming convention\n",
    "    class_date = fileName.split('__')[0]  # split on all instances of __, and keep the first value   \n",
    "    date = class_date.split('_', 1)[-1]  # split on only the first instance of \"_\" and keep the final value\n",
    "    date = date.replace('_', '-')  # make date look more ... nice\n",
    "\n",
    "    # load in the file as a pandas dataframe\n",
    "    current_date_df = pd.read_excel(filePath)\n",
    "        \n",
    "    # retrieve the header row for current file\n",
    "    header_index = identify_header_row(current_date_df)\n",
    "    \n",
    "    # load in the file again, now stating the proper header row\n",
    "    current_date_df = pd.read_excel(filePath, header=header_index, na_values='0')\n",
    "\n",
    "    # the dataframes load in with a score row.. we don't need them\n",
    "    current_date_df = current_date_df.loc[current_date_df['Student Names'] != 'Class Scoring']\n",
    "\n",
    "    # rename netID col\n",
    "    current_date_df['netID'] = current_date_df['My UTC netID is']\n",
    "\n",
    "    # keep only those cols which are necessary, this also allows us to easily re-order the cols\n",
    "    keep_cols = ['netID', 'Student Names']\n",
    "    current_date_df = current_date_df[keep_cols]\n",
    "\n",
    "    # add in the date we derived from the filename\n",
    "    current_date_df[date] = 1\n",
    "\n",
    "    # drop any cols which are fully nan\n",
    "    current_date_df = current_date_df.dropna(axis=1, how='all')\n",
    "\n",
    "    # set utcID col to upper case, this helps align inconsistant casing\n",
    "    current_date_df['netID'] = current_date_df['netID'].str.upper()\n",
    "\n",
    "    # append the current_data_df to the df created to hold results\n",
    "    df = df.append(current_date_df, ignore_index=True, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the df is merged, aggregate details based on pandas 'groupby' function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify if any student names which have no netID saved\n",
    "problemIDs = df['Student Names'].loc[df['netID'].isna()]\n",
    "\n",
    "# if so, solve it with below block\n",
    "if len(problemIDs) > 0:\n",
    "    # fill in nan netIDs with most common (mode) netID from student name groupby object\n",
    "    df['netID'] = df.groupby(['Student Names'])['netID'].apply(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "# verify no remaining problematic IDs exist. If it does, raise an exception\n",
    "problemIDs = df['Student Names'].loc[df['netID'].isna()]\n",
    "if len(problemIDs) > 0:\n",
    "    raise Exception('ProblemIDs still exist!')\n",
    "\n",
    "# fill nans with 0.0\n",
    "df.fillna(0.0, inplace=True)\n",
    "\n",
    "# groupby the netID, and assign the longest string from the group back to Student Names\n",
    "# transforming the groupby objects returns the proper series which can be assigned to the column\n",
    "# this aligns the Student name variants\n",
    "df['Student Names'] = df['Student Names'].groupby(df['netID']).transform(retrieve_longest_str)\n",
    "\n",
    "# filtered list comprehension which returns all df columns excepting student names & netID\n",
    "date_cols = [x for x in df.columns if x not in ['Student Names', 'netID']]\n",
    "\n",
    "# iterate over dates collapsing duplicate names\n",
    "for date in date_cols:\n",
    "    df[date] = df[date].groupby(df['netID']).transform(collapse_date_floats)\n",
    "\n",
    "df.rename({'Student Names': 'name'}, axis=1, inplace=True)    \n",
    "\n",
    "# now drop any duplicate rows\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netID</th>\n",
       "      <th>name</th>\n",
       "      <th>09-12-2019</th>\n",
       "      <th>08-29-2019</th>\n",
       "      <th>09-17-2019</th>\n",
       "      <th>09-19-2019</th>\n",
       "      <th>09-10-2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YMP337</td>\n",
       "      <td>Andrew Turgeson</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FZP281</td>\n",
       "      <td>Godfred Sabbih</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NKN328</td>\n",
       "      <td>Noah Gaston</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCN399</td>\n",
       "      <td>Paul Smith</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LHF446</td>\n",
       "      <td>Samuel Tucker Clark</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SXX597</td>\n",
       "      <td>Yost, Mason</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>QVD441</td>\n",
       "      <td>Caleb, Powell</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CRR434</td>\n",
       "      <td>Vandergriff, Taylor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     netID                 name  09-12-2019  08-29-2019  09-17-2019  \\\n",
       "0   YMP337      Andrew Turgeson         1.0         1.0         1.0   \n",
       "1   FZP281       Godfred Sabbih         1.0         1.0         1.0   \n",
       "2   NKN328          Noah Gaston         1.0         1.0         1.0   \n",
       "3   CCN399           Paul Smith         1.0         1.0         1.0   \n",
       "4   LHF446  Samuel Tucker Clark         1.0         1.0         1.0   \n",
       "6   SXX597          Yost, Mason         1.0         1.0         1.0   \n",
       "7   QVD441        Caleb, Powell         1.0         1.0         1.0   \n",
       "14  CRR434  Vandergriff, Taylor         0.0         1.0         1.0   \n",
       "\n",
       "    09-19-2019  09-10-2019  \n",
       "0          1.0         1.0  \n",
       "1          1.0         1.0  \n",
       "2          0.0         1.0  \n",
       "3          1.0         1.0  \n",
       "4          1.0         1.0  \n",
       "6          1.0         1.0  \n",
       "7          1.0         0.0  \n",
       "14         0.0         1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('midterm-report.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
